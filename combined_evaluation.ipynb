{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined_Evaluation\n",
    "\n",
    "This notebook finds polymer:acronym pairs from documents in a database. These pairs are then used to augment a list of polymer candidates found and then evaluate those candidates against a groundtruth list of polymers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import glob\n",
    "import errno\n",
    "import random\n",
    "import sqlite3\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import pymysql.cursors\n",
    "import gensim, logging\n",
    "from __future__ import division\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from abbreviations import get_abbreviations\n",
    "\n",
    "# Avoid ascii error\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create connection to database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "def connect_to_db():\n",
    "    database = \"db/sentences.db\"\n",
    "    conn = create_connection(database)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to DB\n",
    "def create_connection(db_file):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences are retrieved from database documents and placed in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_for_dois(doi,conn):\n",
    "    sentences = []\n",
    "    cur = conn.cursor()\n",
    "    query = \"select sentence from sentences where docid='%s'\" % (doi)\n",
    "    cur.execute(query)\n",
    "    rows = cur.fetchall()\n",
    "    for row in rows:\n",
    "        sentences.append(row[0].rstrip(\".\"))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polymers(doi_input_file, entire_document):\n",
    "\n",
    "    f = open(doi_input_file,'rb')\n",
    "    lines = f.readlines()\n",
    "    print('Processing %d documents as listed in %s' % (len(lines), doi_input_file))\n",
    "    sentences2 = []\n",
    "    connection = connect_to_db()\n",
    "    \n",
    "    # Process line in prediction file\n",
    "    for line in lines:\n",
    "\n",
    "        # Read the doi\n",
    "        doi = line.split(\"\\t\")[0].strip(\"\\n\")\n",
    "         \n",
    "        if entire_document == 1:\n",
    "            sentences = get_sentences_for_dois(doi,connection)\n",
    "            \n",
    "        for sentence in sentences: \n",
    "            sentences2.append(sentence)\n",
    "\n",
    "    return sentences2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sentences from documents\n",
    "sentences_list = get_polymers('data/classifier_dois.txt', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acronym:Longname pairs are found from the database sentences using get_abbreviations and placed into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dictionary of abbreviations:polymers\n",
    "abbrs = get_abbreviations(sentences_list)\n",
    "print(abbrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of found polymer candidates (polymer_candidates.txt) is augmented by any matching acronyms found in the previous step and the augmented list is rewritten. For example: \n",
    "- If polystyrene is in the list of polymer candidates, the found acronym PS and polystyrene are _both_ added to the new list of polymer candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open file of polymer candidates/create new file\n",
    "\n",
    "# f = open('polymer_candidates.txt','r')\n",
    "f2 = open('amber_second_candidates.txt','w+')\n",
    "# F = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_poly(pl):\n",
    "    new_pl = pl.strip().strip(',').strip('.')\n",
    "    if len(new_pl)>=2 and new_pl[0] == '(' and new_pl[len(new_pl)-1]==')':\n",
    "        new_pl=new_pl.rstrip(')').lstrip('(')\n",
    "    return new_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_items(ifile):\n",
    "    items = []\n",
    "    f = open(ifile)\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        poly = cleanup_poly(line.strip(\"\\n\"))\n",
    "        upoly = u'%s' % (poly)\n",
    "        items.append(upoly)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate candidates combined by /, :, or –\n",
    "\n",
    "def split (jfile):\n",
    "    \n",
    "    candidate_polymers = read_items(jfile)\n",
    "    \n",
    "    candidate_polymers_2 = []\n",
    "    for string in candidate_polymers:\n",
    "        if \":\" in string: \n",
    "            x = string.split(\":\")\n",
    "            for string in x:\n",
    "                line_string = '%s\\n' % (string.strip())\n",
    "                candidate_polymers_2.append(line_string)\n",
    "        if \"/\" in string: \n",
    "            y = string.split(\"/\")\n",
    "            for string in y:\n",
    "                line_string = '%s\\n' % (string.strip())\n",
    "                candidate_polymers_2.append(line_string)\n",
    "        if \"–\" in string: \n",
    "            z = string.split(\"–\")\n",
    "            for string in z:\n",
    "                line_string = '%s\\n' % (string.strip())\n",
    "                candidate_polymers_2.append(line_string)\n",
    "        \n",
    "        \n",
    "    joined_list = ''.join(candidate_polymers_2)\n",
    "    \n",
    "    for line in joined_list: \n",
    "        if line not in candidate_polymers: \n",
    "            candidate_polymers.append(line)\n",
    "\n",
    "    return candidate_polymers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format candidates\n",
    "\n",
    "candidates_list = split('polymer_candidates.txt')\n",
    "\n",
    "\n",
    "formatted_strings = list()\n",
    "for string in candidates_list: \n",
    "    plain_string = string.rstrip('\\n')  \n",
    "    if \"[u'\" in plain_string: \n",
    "        plain_string = plain_string.replace(\"[u'\",\"\")\n",
    "        plain_string = plain_string.replace(\"']\",\"\")\n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "    unicode_string = plain_string.decode('utf8')\n",
    "    formatted_strings.append(unicode_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find and record matching pairs\n",
    "\n",
    "for string in formatted_strings:\n",
    "    f2.write('%s\\n' % (string))\n",
    "    for key in abbrs: \n",
    "        if string in abbrs[key]:\n",
    "            line_acronym = '%s\\n' % (key)\n",
    "            f2.write(line_acronym)\n",
    "        if string in key: \n",
    "            mod = ''.join(abbrs[key])\n",
    "            line_polymer = '%s\\n' % (mod)\n",
    "            f2.write(line_polymer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close files \n",
    "\n",
    "# f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate augmented candidates found against groundtruth polymers from documents in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_items(ifile):\n",
    "    items = []\n",
    "    f = open(ifile)\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        poly = cleanup_poly(line.strip(\"\\n\"))\n",
    "        upoly = u'%s' % (poly)\n",
    "        items.append(upoly)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (candidate_file, groundtruth_file):\n",
    "    \n",
    "    candidate_polymers = read_items(candidate_file)\n",
    "    extracted_polymers = read_items(groundtruth_file)\n",
    "    \n",
    "    print (len(candidate_polymers))\n",
    "    print (len(extracted_polymers))\n",
    "\n",
    "    groundtruth_polymers = [x.strip().lower() for x in extracted_polymers]\n",
    "    groundtruth_polymers = list(set(groundtruth_polymers))\n",
    "    print \"Number of polymer names in the dictionary: \", len(groundtruth_polymers)\n",
    "\n",
    "    candidate_polymers = [x.strip().lower() for x in candidate_polymers]\n",
    "    candidate_polymers = list(set(candidate_polymers))\n",
    "    print \"Number of candidate polymers in this file: \", (len(candidate_polymers))\n",
    "    \n",
    "    # Perform evaluation (compute truth positives, false positives and false negatives)\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    for candidate in candidate_polymers:\n",
    "        if candidate in groundtruth_polymers:\n",
    "            TP = TP + 1\n",
    "        else:\n",
    "            FP = FP + 1\n",
    "\n",
    "    precision = (TP/(TP+FP)*100)\n",
    "\n",
    "    FN = 0\n",
    "    for poly in groundtruth_polymers:\n",
    "        if poly not in candidate_polymers:\n",
    "            FN = FN + 1\n",
    "\n",
    "    recall = (TP/(TP+FN)*100)\n",
    "\n",
    "    print \"True pos: \", TP\n",
    "    print \"False pos: \", FP\n",
    "    print \"Precision: \", precision\n",
    "    print \"Recall: \", recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate('amber_second_candidates.txt', 'evaluation/ground_truth/ground_truth_list_format.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
