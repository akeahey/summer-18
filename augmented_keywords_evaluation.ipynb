{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import glob\n",
    "import errno\n",
    "import spacy\n",
    "import random\n",
    "import sqlite3\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "import pymysql.cursors\n",
    "import gensim, logging\n",
    "from __future__ import division\n",
    "from operator import itemgetter\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from abbreviations import get_abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid ascii error\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv file \n",
    "\n",
    "def get_candidates(model, file_name):\n",
    "\n",
    "    keywords = list()\n",
    "    with open(file_name, 'rb') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='|')\n",
    "        for row in spamreader:\n",
    "            for item in row: \n",
    "                if '\\N' in item: \n",
    "                    pass \n",
    "                elif len(item) == 0: \n",
    "                    pass\n",
    "                else:\n",
    "                    if \";\" in item: \n",
    "                        x = item.split(\";\")\n",
    "                        for item in x:\n",
    "                            line_string = item.strip()\n",
    "                            keywords.append(line_string)\n",
    "                    else: \n",
    "                        keywords.append(item)\n",
    "                        \n",
    "    modified_keywords = list()\n",
    "    for word in keywords: \n",
    "        if word in model.wv.vocab:\n",
    "            modified_keywords.append(word)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return modified_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords = polymers # These already give better numbers than PS and polystyrene after looking at clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "#Adding more words to vocab in order to be ignored\n",
    "# nlp.vocab[\"diblock\"].is_stop = False\n",
    "# nlp.vocab[\"g/mol\"].is_stop = False\n",
    "# nlp.vocab[\"kg/mol\"].is_stop = False\n",
    "# nlp.vocab[\"copolymers\"].is_stop = False\n",
    "# nlp.vocab[\"copolymer\"].is_stop = False\n",
    "# nlp.vocab[\"vpol\"].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_pos(word):\n",
    "    '''Checks if a word ends with a common noun, verb, adverb form'''\n",
    "    ending_with = [\"ity\", \"ize\", \"ized\", \"tion\", \"ly\", \"sion\", \"ure\", \"graph\", \"ing\", \"cal\", \"ent\", \"ed\", \"nt\", \"ic\", \"ies\", \"tions\", \"ar\", \"ous\", \"al\", \"able\", \"nce\", \"try\", \"one\", \"ive\", \"or\", \"sis\", \"ter\", \"ory\", \"mer\", \"imides\"]\n",
    "    for end in ending_with:\n",
    "        if word.endswith(end):\n",
    "            return True\n",
    "            \n",
    "    return False\n",
    "\n",
    "def is_ascii(s):\n",
    "    '''Checks if input is english (filtering out Greek symbols)'''\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "def is_number(n):\n",
    "    try:\n",
    "        float(n)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_candidates(candidates, n, ignore_case):\n",
    "    '''Cleaning up candidates that are not polymer names but appear in the same context'''\n",
    "    \n",
    "    #Identify context words that could be removed\n",
    "    context_file = \"./evaluation/misc/context_words.txt\"\n",
    "    context_words = [line.strip() for line in open(context_file).readlines()]\n",
    "    \n",
    "    #Remove unwanted punctuations\n",
    "    candidates = [(k.strip(',.:-;'),v) for (k,v) in candidates]\n",
    "    \n",
    "    #Remove () from candidates enclosed in parenthesis\n",
    "    for i in range(len(candidates)):\n",
    "        cand = candidates[i][0]\n",
    "        \n",
    "        if cand[0] == '(' and cand[-1] ==')':\n",
    "            updated_candidate = (cand[1:-1], candidates[i][1])\n",
    "            candidates[i] = updated_candidate\n",
    "        \n",
    "    #check if case-sensitivity is required\n",
    "    if ignore_case is True:\n",
    "        candidates = [(str.lower(k),k,v) for (k, v) in candidates]\n",
    "        \n",
    "    #Remove candidate that appear in regular English dictionary  \n",
    "    #But don't remove common polymers \n",
    "    \n",
    "    common_polys = ['polyethylene', 'polyurethane', 'polypropylene', 'polyester', 'PS', 'polystyrene', 'PLA', 'PI', 'PET', 'PVP', 'PEG', 'cellulose', 'PAN', 'methyl'] #These are polymers that could appear within spacy vocab\n",
    "    common_polys = [str.lower(polymer) for polymer in common_polys]\n",
    "    \n",
    "    junk_vals = context_words\n",
    "    for (candidate_lower, candidate, sim) in candidates:\n",
    "        if not is_ascii(candidate_lower):\n",
    "            junk_vals.append(candidate_lower)\n",
    "            \n",
    "        elif (candidate in nlp.vocab) and str.lower(candidate) not in common_polys:\n",
    "            junk_vals.append(candidate_lower)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            junk = False\n",
    "            items = re.split(' |:|;|/',candidate_lower)\n",
    "            for item in items:\n",
    "                    \n",
    "                if item != \"poly\" and is_number(item) is False and (\"standard\" in item or (item in nlp.vocab and item not in common_polys)):\n",
    "                    junk = True\n",
    "                    break\n",
    "                    \n",
    "            if junk is True:\n",
    "                junk_vals.append(candidate_lower)\n",
    "        \n",
    "    \n",
    "    candidates = [(candidate_lower, candidate, sim) for (candidate_lower,candidate, sim) in candidates if candidate_lower not in junk_vals]\n",
    "#     candidates.sort(key=operator.itemgetter(2))\n",
    "    candidates = sorted(candidates, key=lambda x : x[2])\n",
    "    dict_cand = {}\n",
    "    dict_cand_lower = {}\n",
    "    \n",
    "    for cand in candidates:\n",
    "        if cand[0] in dict_cand_lower.keys():\n",
    "            cand_with_case = dict_cand_lower[cand[0]]\n",
    "            dict_cand[cand_with_case] = max(dict_cand[cand_with_case], cand[2])\n",
    "        \n",
    "        else:\n",
    "            dict_cand[cand[1]] = cand[2]\n",
    "            dict_cand_lower[cand[0]] = cand[1] #Mapping lower case to acual polymer\n",
    "    # sort the polymers by similarity similar to the keywords \n",
    "\n",
    "    candidates = sorted(list(dict_cand.items()), key=itemgetter(1), reverse=True) \n",
    "    \n",
    "    # take the top n candidates and generate a list\n",
    "    candidate_list = [cand for (cand,v) in candidates[:n]]\n",
    "    \n",
    "    candidate_list = list(set(candidate_list)) # not really required\n",
    "    return candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates_similar(keyword_list, model, n, ignore_case=True):\n",
    "    ''' Gets candidates that are similar to a list of keywords'''\n",
    "    candidates = []\n",
    "    for keyword in keyword_list:\n",
    "        candidates += model.wv.most_similar(positive=keyword, topn=1500)\n",
    "\n",
    "    candidate_list = clean_up_candidates(candidates, n, ignore_case)\n",
    "    return candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_candidates_to_file(model, input_file_name, file_name, n=800, ignore_case=True):\n",
    "    ''' Gets the list of candidates from get_candidates_similar and writes to file '''\n",
    "    \n",
    "    modified_keywords = get_candidates(model, input_file_name)\n",
    "    cand_list = get_candidates_similar(modified_keywords, model, n, ignore_case)\n",
    "    file = open(file_name, 'w+')\n",
    "    \n",
    "    for cand in cand_list:\n",
    "        file.write(cand + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_FT_model(in_file, sg=1, word_ngrams=5, iter=10, window=5):\n",
    "    ''' Train a FastText skipgram model for word embedding using subword information'''\n",
    "    \n",
    "    f = open(in_file, 'r')\n",
    "    lines = f.readlines()\n",
    "\n",
    "    sentence_token_list = []\n",
    "    for line in lines:\n",
    "        tokens = re.split(r\"\\s+(?=[^()]*(?:\\(|$))\", line.rstrip(\".\"))\n",
    "        tokens = tokens[1:] # First is docid \n",
    "        sentence_token_list.append([token.strip(',.:-; ') for token in tokens])\n",
    "    print (sentence_token_list)  \n",
    "    model = FastText(sentence_token_list, size=120, window=window, min_count=1, workers=4,sg=sg, word_ngrams=word_ngrams, iter=iter, seed=23)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Comment this cell after saving for the first time\n",
    "\n",
    "# model_dir = \"./models\"\n",
    "# if not os.path.exists(model_dir):\n",
    "#     os.makedirs(model_dir)\n",
    "    \n",
    "# model = train_FT_model(\"./data/FT_WE_data_input.txt\", sg=0, word_ngrams=5)#sg = 0 => cbow model\n",
    "# model.save('./models/FT_cbow.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText.load('./models/FT_cbow.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_candidates_to_file(model, \"scraped_polymer_names.txt\", \"./evaluation/candidates/listformat/second_FT_candidates_500.txt\", n=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
